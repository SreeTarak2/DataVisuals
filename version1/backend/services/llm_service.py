import openai
import json
import logging
import requests
import httpx
from typing import Dict, List, Any, Optional
from models import LLMRequest, LLMResponse
from config import settings
import asyncio
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)


class LLMService:
    """Service for interacting with OpenAI API or local LLM for AI-powered insights."""
    
    def __init__(self):
        self.openai_client = None
        self.model = settings.openai_model
        self.executor = ThreadPoolExecutor(max_workers=3)
        self.use_local_llm = False
        
        # Determine which LLM to use based on config
        self.use_local_llm = settings.use_local_llm
        
        # Initialize OpenAI client if API key is available and not using local LLM
        if not self.use_local_llm and settings.openai_api_key and settings.openai_api_key.strip():
            try:
                self.openai_client = openai.OpenAI(
                    api_key=settings.openai_api_key,
                    base_url=settings.openai_base_url
                )
                logger.info("OpenAI client initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize OpenAI client: {e}, falling back to local LLM")
                self.use_local_llm = True
        else:
            if self.use_local_llm:
                logger.info("Using local LLM as configured")
            else:
                logger.info("No OpenAI API key provided, using local LLM fallback")
                self.use_local_llm = True
    
    async def explain_dataset(self, dataset_profile: Dict[str, Any]) -> LLMResponse:
        """Generate AI-powered explanation of the dataset."""
        try:
            # Create explanation prompt
            prompt = self._create_explanation_prompt(dataset_profile)
            
            # Get response from OpenAI or local LLM
            if self.use_local_llm or not self.openai_client:
                response = await self._call_local_llm(prompt)
            else:
                response = await self._call_openai(prompt)
            
            return LLMResponse(
                response=response.get('content', ''),
                confidence=response.get('confidence', 0.9),
                reasoning=response.get('reasoning', 'Generated by AI'),
                suggested_actions=response.get('suggested_actions', []),
            )
            
        except Exception as e:
            logger.error(f"Error explaining dataset: {e}")
            # Fallback to rule-based explanation
            return self._fallback_explanation(dataset_profile)
    
    async def recommend_visualization(self, dataset_profile: Dict[str, Any], 
                                   query: str) -> LLMResponse:
        """Generate AI-powered visualization recommendations."""
        try:
            prompt = self._create_visualization_prompt(dataset_profile, query)
            
            if self.use_local_llm or not self.openai_client:
                response = await self._call_local_llm(prompt)
            else:
                response = await self._call_openai(prompt)
            
            return LLMResponse(
                response=response.get('content', ''),
                confidence=response.get('confidence', 0.9),
                reasoning=response.get('reasoning', 'Generated by AI'),
                suggested_actions=response.get('suggested_actions', []),
            )
            
        except Exception as e:
            logger.error(f"Error recommending visualization: {e}")
            return self._fallback_visualization_recommendation(dataset_profile, query)
    
    async def answer_query(self, request: LLMRequest) -> LLMResponse:
        """Answer natural language queries about the dataset."""
        try:
            # Get dataset profile from database (this would need to be implemented)
            # For now, we'll use a placeholder
            dataset_profile = {"placeholder": "dataset_info"}
            
            prompt = self._create_query_prompt(dataset_profile, request.query)
            
            if self.use_local_llm or not self.openai_client:
                response = await self._call_local_llm(prompt)
            else:
                response = await self._call_openai(prompt)
            
            return LLMResponse(
                response=response.get('content', ''),
                confidence=response.get('confidence', 0.9),
                reasoning=response.get('reasoning', 'Generated by AI'),
                suggested_actions=response.get('suggested_actions', []),
            )
            
        except Exception as e:
            logger.error(f"Error answering query: {e}")
            return self._fallback_query_answer(request)
    
    async def _call_openai(self, prompt: str) -> Dict[str, Any]:
        """Make async call to OpenAI API."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, self._sync_call_openai, prompt)
    
    def _sync_call_openai(self, prompt: str) -> Dict[str, Any]:
        """Synchronous call to OpenAI API."""
        try:
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are DataSage AI, an expert data analyst and visualization specialist. Provide clear, actionable insights and recommendations."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=1000,
                temperature=0.7,
                top_p=0.9
            )
            
            content = response.choices[0].message.content
            
            # Try to parse structured response
            try:
                # Look for JSON in the response
                if '{' in content and '}' in content:
                    start = content.find('{')
                    end = content.rfind('}') + 1
                    json_str = content[start:end]
                    parsed = json.loads(json_str)
                    return {
                        'content': parsed.get('response', content),
                        'confidence': parsed.get('confidence', 0.9),
                        'reasoning': parsed.get('reasoning', 'Generated by OpenAI GPT-4'),
                        'suggested_actions': parsed.get('suggested_actions', [])
                    }
            except json.JSONDecodeError:
                pass
            
            # Return unstructured response
            return {
                'content': content,
                'confidence': 0.9,
                'reasoning': 'Generated by OpenAI GPT-4',
                'suggested_actions': []
            }
            
        except Exception as e:
            logger.error(f"OpenAI API call failed: {e}")
            raise e
    
    async def _call_local_llm(self, prompt: str) -> Dict[str, Any]:
        """Call local LLM service (Ollama or similar)."""
        try:
            # Try Ollama first (most common local LLM)
            response = await self._call_ollama(prompt)
            return response
        except Exception as e:
            logger.warning(f"Ollama call failed: {e}, using rule-based fallback")
            return self._rule_based_response(prompt)
    
    async def _call_ollama(self, prompt: str) -> Dict[str, Any]:
        """Call Ollama local LLM service."""
        try:
            # Use configured model
            model = settings.local_llm_model
            
            payload = {
                "model": model,
                "prompt": f"You are DataSage AI, an expert data analyst. {prompt}",
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": 1000
                }
            }
            
            # Add headers for ngrok
            headers = {
                'Content-Type': 'application/json',
                'ngrok-skip-browser-warning': 'true'
            }
            
            response = requests.post(
                f"{settings.local_llm_url}/api/generate",
                json=payload,
                headers=headers,
                timeout=60  # Increased timeout for remote service
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get("response", "")
                
                # Try to parse structured response
                try:
                    if '{' in content and '}' in content:
                        start = content.find('{')
                        end = content.rfind('}') + 1
                        json_str = content[start:end]
                        parsed = json.loads(json_str)
                        return {
                            'content': parsed.get('response', content),
                            'confidence': parsed.get('confidence', 0.8),
                            'reasoning': parsed.get('reasoning', 'Generated by local LLM'),
                            'suggested_actions': parsed.get('suggested_actions', [])
                        }
                except json.JSONDecodeError:
                    pass
                
                return {
                    'content': content,
                    'confidence': 0.8,
                    'reasoning': 'Generated by local LLM (Ollama)',
                    'suggested_actions': []
                }
            else:
                raise Exception(f"Ollama API returned status {response.status_code}")
                
        except Exception as e:
            logger.error(f"Ollama call failed: {e}")
            raise e
    
    def _rule_based_response(self, prompt: str) -> Dict[str, Any]:
        """Generate rule-based response when LLM is not available."""
        # Simple keyword-based responses
        prompt_lower = prompt.lower()
        
        if "visualization" in prompt_lower or "chart" in prompt_lower:
            content = """Based on your data, I recommend the following visualizations:
            
1. **Bar Charts** - For categorical data comparisons
2. **Line Charts** - For time series and trends
3. **Scatter Plots** - For correlation analysis
4. **Histograms** - For distribution analysis
5. **Heatmaps** - For correlation matrices

Choose the chart type that best represents your data relationships."""
            
        elif "explain" in prompt_lower or "analysis" in prompt_lower:
            content = """Here's a basic analysis of your dataset:

**Data Overview:**
- The dataset appears to be structured and ready for analysis
- Consider checking data quality and completeness
- Look for patterns and relationships between variables

**Next Steps:**
1. Examine data distributions
2. Check for missing values
3. Identify key variables
4. Look for correlations
5. Create appropriate visualizations"""
            
        else:
            content = """I can help you analyze your dataset. Here are some common tasks:

1. **Data Exploration** - Understand your data structure and quality
2. **Visualization** - Create charts and graphs to reveal patterns
3. **Statistical Analysis** - Find correlations and trends
4. **Data Cleaning** - Handle missing values and outliers
5. **Insights Generation** - Extract meaningful business insights

Please specify what you'd like to analyze or visualize."""
        
        return {
            'content': content,
            'confidence': 0.6,
            'reasoning': 'Rule-based response (no LLM available)',
            'suggested_actions': ['Upload data for detailed analysis', 'Try specific analysis requests']
        }
    
    
    
    def _create_visualization_prompt(self, profile: Dict[str, Any], query: str) -> str:
        """Create prompt for visualization recommendations."""
        context = "simple, business-friendly, and technical"
        
        return f"""
        You are a data visualization expert recommending charts for a dataset.
        
        User Query: {query}
        Context: {context}
        
        Dataset Profile:
        - Columns: {profile.get('column_count', 'Unknown')}
        - Column Types: {self._get_column_type_summary(profile.get('columns', []))}
        
        Please provide a JSON response with the following structure:
        {{
            "response": "Your visualization recommendations here",
            "confidence": 0.9,
            "reasoning": "Why these charts are appropriate",
            "suggested_actions": ["action1", "action2"]
        }}
        
        Your recommendations should:
        1. Suggest the most appropriate chart type(s)
        2. Explain why each chart is suitable
        3. Recommend specific fields to use
        4. Include any data preprocessing needed
        5. Adapt to the user's persona ({persona_context})
        """
    
    def _create_query_prompt(self, profile: Dict[str, Any], query: str) -> str:
        """Create prompt for answering queries."""
        context = "clear and comprehensive explanations"
        
        return f"""
        You are a data analyst answering questions about a dataset.
        
        User Question: {query}
        Context: {context}
        
        Dataset Context:
        {self._format_columns_for_prompt(profile.get('columns', []))}
        
        Please provide a JSON response with the following structure:
        {{
            "response": "Your answer here",
            "confidence": 0.9,
            "reasoning": "How you arrived at this answer",
            "suggested_actions": ["action1", "action2"]
        }}
        
        Your answer should:
        1. Use {persona_context} appropriate for the user
        2. Be based on the available data
        3. Explain any limitations or assumptions
        4. Suggest additional data if needed
        5. Provide actionable insights
        """
    
    def _format_columns_for_prompt(self, columns: List[Dict[str, Any]]) -> str:
        """Format column information for prompt."""
        if not columns:
            return "No column information available"
        
        formatted = []
        for col in columns[:10]:  # Limit to first 10 columns
            col_info = f"- {col['name']}: {col['dtype']}"
            if col.get('is_numeric'):
                col_info += f" (numeric, nulls: {col.get('null_count', 0)})"
            elif col.get('is_temporal'):
                col_info += " (temporal)"
            elif col.get('is_categorical'):
                col_info += f" (categorical, {col.get('unique_count', 0)} unique values)"
            formatted.append(col_info)
        
        return "\n".join(formatted)
    
    def _format_summary_for_prompt(self, summary: Dict[str, Any]) -> str:
        """Format summary statistics for prompt."""
        if not summary:
            return "No summary statistics available"
        
        return f"""
        - Total cells: {summary.get('total_cells', 'Unknown')}
        - Missing data: {summary.get('missing_percentage', 'Unknown')}%
        - Duplicate rows: {summary.get('duplicate_percentage', 'Unknown')}%
        - Strong correlations: {len(summary.get('strong_correlations', []))}
        """
    
    def _get_column_type_summary(self, columns: List[Dict[str, Any]]) -> str:
        """Get summary of column types."""
        type_counts = {}
        for col in columns:
            if col.get('is_numeric'):
                type_counts['numeric'] = type_counts.get('numeric', 0) + 1
            elif col.get('is_temporal'):
                type_counts['temporal'] = type_counts.get('temporal', 0) + 1
            elif col.get('is_categorical'):
                type_counts['categorical'] = type_counts.get('categorical', 0) + 1
            else:
                type_counts['text'] = type_counts.get('text', 0) + 1
        
        return ", ".join([f"{count} {type_}" for type_, count in type_counts.items()])
    
    def _fallback_explanation(self, profile: Dict[str, Any]) -> LLMResponse:
        """Fallback explanation when OpenAI fails."""
        response = f"This dataset contains {profile.get('row_count', 'unknown number of')} rows and {profile.get('column_count', 'unknown number of')} columns. It appears to be a structured dataset that could be useful for analysis. Data quality score: {profile.get('data_quality', {}).get('overall_score', 'N/A')}/100."
        
        return LLMResponse(
            response=response,
            confidence=0.6,
            reasoning="Fallback rule-based explanation",
            suggested_actions=["Upload data for detailed analysis", "Check data quality"],
        )
    
    def _fallback_visualization_recommendation(self, profile: Dict[str, Any], query: str) -> LLMResponse:
        """Fallback visualization recommendation when OpenAI fails."""
        response = "Based on the data structure, I recommend starting with basic charts like bar charts for categorical data and line charts for time series data."
        
        return LLMResponse(
            response=response,
            confidence=0.5,
            reasoning="Fallback rule-based recommendation",
            suggested_actions=["Try basic chart types", "Check data types"],
        )
    
    def _fallback_query_answer(self, request: LLMRequest) -> LLMResponse:
        """Fallback answer when OpenAI fails."""
        response = "I'm unable to process your query at the moment. Please try rephrasing or check if the dataset is properly loaded."
        
        return LLMResponse(
            response=response,
            confidence=0.3,
            reasoning="OpenAI service unavailable",
            suggested_actions=["Check dataset status", "Try simpler query"],
        )
    
    async def generate_questions(self, prompt: str) -> str:
        """Generate questions using LLM for QUIS-style analysis."""
        try:
            if self.use_local_llm:
                return await self._generate_questions_local(prompt)
            else:
                return await self._generate_questions_openai(prompt)
        except Exception as e:
            logger.error(f"Error generating questions: {e}")
            raise e
    
    async def _generate_questions_local(self, prompt: str) -> str:
        """Generate questions using local LLM."""
        try:
            headers = {
                "Content-Type": "application/json",
            }
            
            data = {
                "model": settings.local_llm_model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": 2000
                }
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{settings.local_llm_url}api/generate",
                    headers=headers,
                    json=data,
                    timeout=30.0
                )
                response.raise_for_status()
                result = response.json()
                
                return result.get("response", "")
                
        except Exception as e:
            logger.error(f"Error generating questions with local LLM: {e}")
            return self._fallback_question_generation()
    
    async def _generate_questions_openai(self, prompt: str) -> str:
        """Generate questions using OpenAI."""
        try:
            headers = {
                "Authorization": f"Bearer {settings.openai_api_key}",
                "Content-Type": "application/json",
            }
            
            data = {
                "model": settings.openai_model,
                "messages": [
                    {
                        "role": "system",
                        "content": "You are an expert data analyst generating insightful questions for automated exploratory data analysis. Always respond with valid JSON format."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.7,
                "max_tokens": 2000
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{settings.openai_base_url}/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=30.0
                )
                response.raise_for_status()
                result = response.json()
                
                return result["choices"][0]["message"]["content"]
                
        except Exception as e:
            logger.error(f"Error generating questions with OpenAI: {e}")
            return self._fallback_question_generation()
    
    def _fallback_question_generation(self) -> str:
        """Fallback question generation when LLM fails."""
        return """
{
  "insight_cards": [
    {
      "question": "What are the main patterns in this dataset?",
      "reason": "This question helps identify overall trends and distributions in the data",
      "breakdown": "category",
      "measure": "count"
    },
    {
      "question": "Which categories show the highest values?",
      "reason": "This question reveals the top-performing categories or segments",
      "breakdown": "category", 
      "measure": "sum"
    }
  ]
}
"""