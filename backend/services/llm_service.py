import openai
import json
import logging
from typing import Dict, List, Any, Optional
from models import PersonaType, LLMRequest, LLMResponse
from config import settings
import asyncio
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)


class LLMService:
    """Service for interacting with OpenAI API for AI-powered insights."""
    
    def __init__(self):
        self.client = openai.OpenAI(
            api_key=settings.openai_api_key,
            base_url=settings.openai_base_url
        )
        self.model = settings.openai_model
        self.executor = ThreadPoolExecutor(max_workers=3)
    
    async def explain_dataset(self, dataset_profile: Dict[str, Any], persona: PersonaType) -> LLMResponse:
        """Generate AI-powered explanation of the dataset."""
        try:
            # Create prompt based on persona
            if persona == PersonaType.NORMAL:
                prompt = self._create_normal_persona_prompt(dataset_profile)
            else:
                prompt = self._create_expert_persona_prompt(dataset_profile)
            
            # Get OpenAI response
            response = await self._call_openai(prompt)
            
            return LLMResponse(
                response=response.get('content', ''),
                confidence=response.get('confidence', 0.9),
                reasoning=response.get('reasoning', 'Generated by OpenAI GPT-4'),
                suggested_actions=response.get('suggested_actions', []),
                persona_adapted=True
            )
            
        except Exception as e:
            logger.error(f"Error explaining dataset: {e}")
            # Fallback to rule-based explanation
            return self._fallback_explanation(dataset_profile, persona)
    
    async def recommend_visualization(self, dataset_profile: Dict[str, Any], 
                                   query: str, persona: PersonaType) -> LLMResponse:
        """Generate AI-powered visualization recommendations."""
        try:
            prompt = self._create_visualization_prompt(dataset_profile, query, persona)
            response = await self._call_openai(prompt)
            
            return LLMResponse(
                response=response.get('content', ''),
                confidence=response.get('confidence', 0.9),
                reasoning=response.get('reasoning', 'Generated by OpenAI GPT-4'),
                suggested_actions=response.get('suggested_actions', []),
                persona_adapted=True
            )
            
        except Exception as e:
            logger.error(f"Error recommending visualization: {e}")
            return self._fallback_visualization_recommendation(dataset_profile, query, persona)
    
    async def answer_query(self, request: LLMRequest) -> LLMResponse:
        """Answer natural language queries about the dataset."""
        try:
            # Get dataset profile from database (this would need to be implemented)
            # For now, we'll use a placeholder
            dataset_profile = {"placeholder": "dataset_info"}
            
            prompt = self._create_query_prompt(dataset_profile, request.query, request.persona)
            response = await self._call_openai(prompt)
            
            return LLMResponse(
                response=response.get('content', ''),
                confidence=response.get('confidence', 0.9),
                reasoning=response.get('reasoning', 'Generated by OpenAI GPT-4'),
                suggested_actions=response.get('suggested_actions', []),
                persona_adapted=True
            )
            
        except Exception as e:
            logger.error(f"Error answering query: {e}")
            return self._fallback_query_answer(request)
    
    async def _call_openai(self, prompt: str) -> Dict[str, Any]:
        """Make async call to OpenAI API."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, self._sync_call_openai, prompt)
    
    def _sync_call_openai(self, prompt: str) -> Dict[str, Any]:
        """Synchronous call to OpenAI API."""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are DataSage AI, an expert data analyst and visualization specialist. Provide clear, actionable insights and recommendations."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=1000,
                temperature=0.7,
                top_p=0.9
            )
            
            content = response.choices[0].message.content
            
            # Try to parse structured response
            try:
                # Look for JSON in the response
                if '{' in content and '}' in content:
                    start = content.find('{')
                    end = content.rfind('}') + 1
                    json_str = content[start:end]
                    parsed = json.loads(json_str)
                    return {
                        'content': parsed.get('response', content),
                        'confidence': parsed.get('confidence', 0.9),
                        'reasoning': parsed.get('reasoning', 'Generated by OpenAI GPT-4'),
                        'suggested_actions': parsed.get('suggested_actions', [])
                    }
            except json.JSONDecodeError:
                pass
            
            # Return unstructured response
            return {
                'content': content,
                'confidence': 0.9,
                'reasoning': 'Generated by OpenAI GPT-4',
                'suggested_actions': []
            }
            
        except Exception as e:
            logger.error(f"OpenAI API call failed: {e}")
            raise e
    
    def _create_normal_persona_prompt(self, profile: Dict[str, Any]) -> str:
        """Create prompt for normal persona."""
        return f"""
        You are a helpful data analyst explaining a dataset in simple, business-friendly terms.
        
        Dataset Summary:
        - Rows: {profile.get('row_count', 'Unknown')}
        - Columns: {profile.get('column_count', 'Unknown')}
        - Data Quality Score: {profile.get('data_quality', {}).get('overall_score', 'Unknown')}
        
        Column Information:
        {self._format_columns_for_prompt(profile.get('columns', []))}
        
        Please provide a JSON response with the following structure:
        {{
            "response": "Your explanation here",
            "confidence": 0.9,
            "reasoning": "Why this explanation is useful",
            "suggested_actions": ["action1", "action2"]
        }}
        
        Your explanation should:
        1. Use simple, business-friendly language
        2. Avoid technical jargon
        3. Focus on business value and insights
        4. Suggest actionable next steps
        5. Be clear and concise
        """
    
    def _create_expert_persona_prompt(self, profile: Dict[str, Any]) -> str:
        """Create prompt for expert persona."""
        return f"""
        You are a senior data scientist providing technical analysis of a dataset.
        
        Dataset Summary:
        - Rows: {profile.get('row_count', 'Unknown')}
        - Columns: {profile.get('column_count', 'Unknown')}
        - Memory Usage: {profile.get('memory_usage', 'Unknown')} bytes
        - Data Quality Score: {profile.get('data_quality', {}).get('overall_score', 'Unknown')}
        
        Column Analysis:
        {self._format_columns_for_prompt(profile.get('columns', []))}
        
        Statistical Summary:
        {self._format_summary_for_prompt(profile.get('summary_stats', {}))}
        
        Please provide a JSON response with the following structure:
        {{
            "response": "Your technical analysis here",
            "confidence": 0.9,
            "reasoning": "Statistical reasoning and methodology",
            "suggested_actions": ["action1", "action2"]
        }}
        
        Your analysis should:
        1. Use technical terminology and statistical measures
        2. Include confidence levels and significance testing
        3. Provide advanced pattern recognition insights
        4. Suggest analytical methodologies
        5. Include statistical validation approaches
        """
    
    def _create_visualization_prompt(self, profile: Dict[str, Any], query: str, persona: PersonaType) -> str:
        """Create prompt for visualization recommendations."""
        persona_context = "simple, business-friendly" if persona == PersonaType.NORMAL else "technical, statistical"
        
        return f"""
        You are a data visualization expert recommending charts for a dataset.
        
        User Query: {query}
        Persona: {persona_context}
        
        Dataset Profile:
        - Columns: {profile.get('column_count', 'Unknown')}
        - Column Types: {self._get_column_type_summary(profile.get('columns', []))}
        
        Please provide a JSON response with the following structure:
        {{
            "response": "Your visualization recommendations here",
            "confidence": 0.9,
            "reasoning": "Why these charts are appropriate",
            "suggested_actions": ["action1", "action2"]
        }}
        
        Your recommendations should:
        1. Suggest the most appropriate chart type(s)
        2. Explain why each chart is suitable
        3. Recommend specific fields to use
        4. Include any data preprocessing needed
        5. Adapt to the user's persona ({persona_context})
        """
    
    def _create_query_prompt(self, profile: Dict[str, Any], query: str, persona: PersonaType) -> str:
        """Create prompt for answering queries."""
        persona_context = "simple explanations" if persona == PersonaType.NORMAL else "technical analysis"
        
        return f"""
        You are a data analyst answering questions about a dataset.
        
        User Question: {query}
        Persona: {persona_context}
        
        Dataset Context:
        {self._format_columns_for_prompt(profile.get('columns', []))}
        
        Please provide a JSON response with the following structure:
        {{
            "response": "Your answer here",
            "confidence": 0.9,
            "reasoning": "How you arrived at this answer",
            "suggested_actions": ["action1", "action2"]
        }}
        
        Your answer should:
        1. Use {persona_context} appropriate for the user
        2. Be based on the available data
        3. Explain any limitations or assumptions
        4. Suggest additional data if needed
        5. Provide actionable insights
        """
    
    def _format_columns_for_prompt(self, columns: List[Dict[str, Any]]) -> str:
        """Format column information for prompt."""
        if not columns:
            return "No column information available"
        
        formatted = []
        for col in columns[:10]:  # Limit to first 10 columns
            col_info = f"- {col['name']}: {col['dtype']}"
            if col.get('is_numeric'):
                col_info += f" (numeric, nulls: {col.get('null_count', 0)})"
            elif col.get('is_temporal'):
                col_info += " (temporal)"
            elif col.get('is_categorical'):
                col_info += f" (categorical, {col.get('unique_count', 0)} unique values)"
            formatted.append(col_info)
        
        return "\n".join(formatted)
    
    def _format_summary_for_prompt(self, summary: Dict[str, Any]) -> str:
        """Format summary statistics for prompt."""
        if not summary:
            return "No summary statistics available"
        
        return f"""
        - Total cells: {summary.get('total_cells', 'Unknown')}
        - Missing data: {summary.get('missing_percentage', 'Unknown')}%
        - Duplicate rows: {summary.get('duplicate_percentage', 'Unknown')}%
        - Strong correlations: {len(summary.get('strong_correlations', []))}
        """
    
    def _get_column_type_summary(self, columns: List[Dict[str, Any]]) -> str:
        """Get summary of column types."""
        type_counts = {}
        for col in columns:
            if col.get('is_numeric'):
                type_counts['numeric'] = type_counts.get('numeric', 0) + 1
            elif col.get('is_temporal'):
                type_counts['temporal'] = type_counts.get('temporal', 0) + 1
            elif col.get('is_categorical'):
                type_counts['categorical'] = type_counts.get('categorical', 0) + 1
            else:
                type_counts['text'] = type_counts.get('text', 0) + 1
        
        return ", ".join([f"{count} {type_}" for type_, count in type_counts.items()])
    
    def _fallback_explanation(self, profile: Dict[str, Any], persona: PersonaType) -> LLMResponse:
        """Fallback explanation when OpenAI fails."""
        if persona == PersonaType.NORMAL:
            response = f"This dataset contains {profile.get('row_count', 'unknown number of')} rows and {profile.get('column_count', 'unknown number of')} columns. It appears to be a structured dataset that could be useful for analysis."
        else:
            response = f"Dataset analysis: {profile.get('row_count', 'N/A')} observations across {profile.get('column_count', 'N/A')} variables. Data quality score: {profile.get('data_quality', {}).get('overall_score', 'N/A')}/100."
        
        return LLMResponse(
            response=response,
            confidence=0.6,
            reasoning="Fallback rule-based explanation",
            suggested_actions=["Upload data for detailed analysis", "Check data quality"],
            persona_adapted=False
        )
    
    def _fallback_visualization_recommendation(self, profile: Dict[str, Any], query: str, persona: PersonaType) -> LLMResponse:
        """Fallback visualization recommendation when OpenAI fails."""
        response = "Based on the data structure, I recommend starting with basic charts like bar charts for categorical data and line charts for time series data."
        
        return LLMResponse(
            response=response,
            confidence=0.5,
            reasoning="Fallback rule-based recommendation",
            suggested_actions=["Try basic chart types", "Check data types"],
            persona_adapted=False
        )
    
    def _fallback_query_answer(self, request: LLMRequest) -> LLMResponse:
        """Fallback answer when OpenAI fails."""
        response = "I'm unable to process your query at the moment. Please try rephrasing or check if the dataset is properly loaded."
        
        return LLMResponse(
            response=response,
            confidence=0.3,
            reasoning="OpenAI service unavailable",
            suggested_actions=["Check dataset status", "Try simpler query"],
            persona_adapted=False
        )
